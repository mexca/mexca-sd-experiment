{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Performance of Speaker Diarization Pipeline on DED21 Dataset\n",
    "\n",
    "This notebook analyzes the performance of two speaker diarization pipelines on the DED21 dataset (see this [blog post](https://stukroodvlees.nl/welke-lijsttrekkers-lacht-het-meest-en-hoe/) for details). The two pipelines were the ones performing best on the AMI corpus:\n",
    "\n",
    "1. Custom pipeline:\n",
    "- pyannotes [speaker-segmentation](https://huggingface.co/pyannote/speaker-segmentation) pipeline for detecting speaker segments (voice activity + speaker change + overlapped speech) with postprocessing steps (merging close and removing short segments)\n",
    "- Speechbrains [pretrained ECAPA-TDNN](https://huggingface.co/speechbrain/spkrec-ecapa-voxceleb) model with default settings as in `model.encode_batch()` for speaker embeddings\n",
    "- scikit-learns spectral clustering algorithm for speaker segment clustering\n",
    "\n",
    "2. pyannotes full [speaker-diarization](https://huggingface.co/pyannote/speaker-diarization) pipeline (combines all steps from the custom pipeline)\n",
    "\n",
    "The performance of the pipelines is assessed by different metrics. See the [documentation](https://pyannote.github.io/pyannote-metrics/reference) of `pyannote.metrics` for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MalteLuken\\Repositories\\mexca-sd-experiment\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "from custom_datasets import load_ded21_dataset\n",
    "from pyannote.core import Annotation, Segment\n",
    "from pyannote.metrics.diarization import DiarizationErrorRate, DiarizationCoverage, DiarizationPurity\n",
    "from pyannote.metrics.segmentation import SegmentationPrecision, SegmentationRecall, SegmentationPurity, SegmentationCoverage\n",
    "from pyannote.metrics.detection import DetectionErrorRate\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from speaker_representation import load_speech_sequence\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.join(\"..\", \"dutch-debate-corpus\")\n",
    "DATASET = \"ded21\"\n",
    "MODEL = \"speechbrain-ecapa-tdnn\"\n",
    "SEG_DIR = os.path.join(\"..\", \"speaker-segmentation\", \"results\", \"pyannote\")\n",
    "RESULTS_DIR = os.path.join(\"results\", MODEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_subsegment_embeddings(rttm_seq, embeddings):\n",
    "    splits = []\n",
    "\n",
    "    for seg in rttm_seq.sequence:\n",
    "        splits.append(seg.tdur // 20.0 + 1)\n",
    "\n",
    "    new_embeddings = torch.empty((len(splits), embeddings.shape[1]))\n",
    "\n",
    "    for j, n in enumerate(splits):\n",
    "        new_embedding = torch.mean(embeddings[j:(j+int(n)), :], dim=0)\n",
    "        new_embeddings[j, :] = torch.nn.functional.normalize(\n",
    "            new_embedding, dim=-1).cpu()\n",
    "\n",
    "    assert new_embeddings.shape[0] == len(splits)\n",
    "\n",
    "    return new_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reference_speakers(ref_dir, dataset, index):\n",
    "    rttm_seq = load_speech_sequence(ref_dir, dataset, index)\n",
    "    speakers = [seg.name for seg in rttm_seq.sequence]\n",
    "    return list(set(speakers))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_speaker_embeddings(model, dataset, index):\n",
    "    with os.scandir(os.path.join(\"embeddings\", model)) as filenames:\n",
    "        for filename in filenames:\n",
    "            if filename.name.split(\"_\")[-1].find(str(index)) != -1 and filename.name.find(dataset) != -1:\n",
    "                print(filename.path)\n",
    "                embeddings = torch.load(filename.path)\n",
    "                return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_rttm_annotation(rttm_seq):\n",
    "    annotation = Annotation()\n",
    "    for seg in rttm_seq.sequence:\n",
    "        annotation[Segment(seg.tbeg, seg.tbeg+seg.tdur)] = seg.name\n",
    "\n",
    "    return annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings\\speechbrain-ecapa-tdnn\\sb_ecapa_tdnn_pa-seg_ded21_0.pt\n",
      "embeddings\\speechbrain-ecapa-tdnn\\sb_ecapa_tdnn_pa-seg_ded21_1.pt\n"
     ]
    }
   ],
   "source": [
    "dataset = load_ded21_dataset(DATA_DIR)\n",
    "\n",
    "rttm_seqs = []\n",
    "embeddings_combined = []\n",
    "speaker_labels = []\n",
    "\n",
    "for i, sample in enumerate(dataset):\n",
    "    speaker_labels += get_reference_speakers(DATA_DIR, DATASET, i)\n",
    "    embeddings = load_speaker_embeddings(MODEL, DATASET, i)\n",
    "    rttm_seq = load_speech_sequence(SEG_DIR, DATASET, i)\n",
    "    new_embeddings = average_subsegment_embeddings(rttm_seq, embeddings)\n",
    "    embeddings_combined += new_embeddings.squeeze()\n",
    "    rttm_seqs.append(rttm_seq)\n",
    "\n",
    "speaker_labels = list(set(speaker_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rttm_seq_combined = deepcopy(rttm_seqs[0])\n",
    "\n",
    "for seq in rttm_seqs[1:]:\n",
    "    rttm_seq_combined.append(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "classifier = SpectralClustering(len(speaker_labels))\n",
    "num_labels = classifier.fit_predict(np.array([np.array(emb) for emb in embeddings_combined]))\n",
    "\n",
    "for i, seg in enumerate(rttm_seq_combined.sequence):\n",
    "    seg.name = num_labels[i]\n",
    "\n",
    "for i, seg in enumerate(rttm_seqs[0].sequence + rttm_seqs[1].sequence):\n",
    "    seg.name = num_labels[i]\n",
    "\n",
    "for i, seq in enumerate(rttm_seqs):\n",
    "    seq.write(os.path.join(\"results\", MODEL, f\"sc_sb_ecapa_tdnn_{DATASET}_sample_{i}.rttm\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_annotation = convert_rttm_annotation(rttm_seq_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_rttm_seq_combined = load_speech_sequence(DATA_DIR, DATASET, 0)\n",
    "\n",
    "for i, _ in enumerate(dataset):\n",
    "    if i > 0:\n",
    "        ref_rttm_seq_combined.append(load_speech_sequence(DATA_DIR, DATASET, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_annotation = convert_rttm_annotation(ref_rttm_seq_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "diarization_der = DiarizationErrorRate(collar=0.25)\n",
    "diarization_coverage = DiarizationCoverage(collar=0.25)\n",
    "diarization_purity = DiarizationPurity(collar=0.25)\n",
    "diarization_mapping = diarization_der.optimal_mapping\n",
    "vad_detection = DetectionErrorRate(collar=0.25)\n",
    "seg_precision = SegmentationPrecision(tolerance=0.5)\n",
    "seg_recall = SegmentationRecall(tolerance=0.5)\n",
    "seg_coverage = SegmentationCoverage()\n",
    "seg_purity = SegmentationPurity()\n",
    "\n",
    "metrics = {\n",
    "    \"der\": diarization_der, \"d_cov\": diarization_coverage, \n",
    "    \"d_pur\": diarization_purity, \"d_map\": diarization_mapping,\n",
    "    \"vad_err\": vad_detection, \"seg_pre\": seg_precision, \"seg_rec\": seg_recall,\n",
    "    \"seg_cov\": seg_coverage, \"seg_pur\": seg_purity\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MalteLuken\\Repositories\\mexca-sd-experiment\\lib\\site-packages\\pyannote\\metrics\\utils.py:183: UserWarning: 'uem' was approximated by the union of 'reference' and 'hypothesis' extents.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "der: 0.7539607638662575\n",
      "d_cov: 0.33392985478827514\n",
      "d_pur: 0.3381564995009985\n",
      "d_map: {0: 'Klaver', 1: 'Burger', 2: 'Hoekstra', 3: 'Marijnissen', 4: 'Rutte', 5: 'Kaag', 6: 'Wilders'}\n",
      "vad_err: 0.16882226422472735\n",
      "seg_pre: 0.1884498480243161\n",
      "seg_rec: 0.5740740740740741\n",
      "seg_cov: 0.6265871799003123\n",
      "seg_pur: 1.0\n"
     ]
    }
   ],
   "source": [
    "for met in metrics.keys():\n",
    "    print(f\"{met}: {metrics[met](ref_annotation, model_annotation)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metrics indicate that the custom pipeline performs not very well on the DED21 dataset. The VAD error rate is relatively low (although it could be even lower), so VAD does not seem to be the problem. The metrics for speaker change detection (`seg_`) also show an acceptable performance of the pipeline. However, the diarization metrics (`der`, `d_cov`, `d_pur`) signal that the speaker embeddings or the clustering performed inadequately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0:\n",
      "der: 0.35585114806017526\n",
      "d_cov: 0.9542816997557668\n",
      "d_pur: 0.6853093627762681\n",
      "d_map: {'SPEAKER_00': 'Marijnissen', 'SPEAKER_01': 'Kaag', 'SPEAKER_03': 'Rutte', 'SPEAKER_04': 'Hoekstra'}\n",
      "vad_err: 0.04241646872525815\n",
      "seg_pre: 0.18085106382978725\n",
      "seg_rec: 0.7727272727272727\n",
      "seg_cov: 0.599764734004743\n",
      "seg_pur: 1.0\n",
      "Sample 1:\n",
      "der: 0.3851063358440405\n",
      "d_cov: 0.9198759029869336\n",
      "d_pur: 0.6738817274239243\n",
      "d_map: {'SPEAKER_00': 'Kaag', 'SPEAKER_01': 'Klaver', 'SPEAKER_02': 'Marijnissen', 'SPEAKER_03': 'Burger', 'SPEAKER_04': 'Rutte', 'SPEAKER_05': 'Wilders'}\n",
      "vad_err: 0.025117412494461443\n",
      "seg_pre: 0.2084942084942085\n",
      "seg_rec: 0.8571428571428571\n",
      "seg_cov: 0.616670700847178\n",
      "seg_pur: 0.9999999999999998\n"
     ]
    }
   ],
   "source": [
    "for i, sample in enumerate(dataset):\n",
    "    reference = convert_rttm_annotation(load_speech_sequence(DATA_DIR, DATASET, i))\n",
    "    hypothesis = convert_rttm_annotation(load_speech_sequence(os.path.join(\"results\", \"pyannote\"), DATASET, i))\n",
    "    print(f\"Sample {i}:\")\n",
    "    for met in metrics.keys():\n",
    "        print(f\"{met}: {metrics[met](reference, hypothesis)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the pyannote pipeline, the metrics suggest a much better and adequate performance. The VAD error rate is very low, whereas the speaker change detection metrics are comparable to the ones for the custom pipeline. The speaker diarization metrics indicate moderate performance, suggesting that the embedding and clustering worked much better than for the custom pipeline. The higher coverage compared to the purity suggests that the pipeline slightly oversegments the speech signal (i.e., it merges multiple reference speaker into one hypothesized speaker). This can be explained by the lower number of detected speakers compared to the reference."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b6f083d5ac60cf7759826377c3d8f491f9a0109c460299bd7207c8484c8eacf0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('mexca-sd-experiment': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
